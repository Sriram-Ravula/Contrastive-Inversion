#seed - SAME BETWEEN RUNS
seed: 1234

#data - SAME BETWEEN RUNS
dataset_dir: "/tmp/ImageNet100B/"
dataset: "ImageNet100B"
num_classes: 100
emb_dim: 512

#distributed training - SAME BETWEEN RUNS
num_nodes: 1
gpus: 4
workers: 12
distributed_backend: "ddp"

###############################################

#Experiment Stuff - CHANGE BETWEEN RUNS
experiment_name: "clip_st_randmany_100b"
checkpoint_path: "/work2/08002/gsmyrnis/maverick2/clip_experiments/code/Logs_RN101_Adam/randmany_st/checkpoints/epoch=24-step=12374.ckpt"

#Distortions and parameters - CHANGE BETWEEN RUNS
distortion: "randommask" #randommask or squaremask
percent_missing: [0.5,0.95] #only for randommask -THIS IS A FLOAT, PUT IN E.G. 0.9 FOR () percent missing
fixed_mask: False

encoder: "clip"
saved_model_type: "contrastive"

#training
max_epochs: 10
lr: 0.001
batch_size: 128 #100 for clip RN50, 150 for clip Vit, 64 for clip RN101,
use_subset: False

#logging
log_save_interval: 10 #set this to be equal to num_epochs if row_log_interval is 1
row_log_interval: 1
logdir: "/tmp/Logs"
results_dir: "./results"

#validation
check_val_every_n_epoch: 1
